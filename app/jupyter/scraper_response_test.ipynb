{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "print('=== current_dir ',current_dir)\n",
    "\n",
    "launch = os.path.join(current_dir, '.vscode', 'jupyter_env.json')\n",
    "with open(launch, 'r') as f:\n",
    "    file = f.read()\n",
    "launch_json: dict = json.loads(file)\n",
    "\n",
    "for key,value in launch_json.items():\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "from typing import Any\n",
    "from logging import Logger\n",
    "from datetime import datetime\n",
    "from importlib import import_module\n",
    "from pymongo import ASCENDING\n",
    "from pymongo.cursor import Cursor\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "from bs4.element import Tag\n",
    "from bs4.element import ResultSet\n",
    "import json\n",
    "\n",
    "from BrownieAtelierMongo.models.mongo_model import MongoModel\n",
    "from BrownieAtelierMongo.models.crawler_response_model import CrawlerResponseModel\n",
    "from BrownieAtelierMongo.models.scraped_from_response_model import ScrapedFromResponseModel\n",
    "from BrownieAtelierMongo.models.scraper_info_by_domain_model import ScraperInfoByDomainModel\n",
    "from BrownieAtelierMongo.models.controller_model import ControllerModel\n",
    "from shared.timezone_recovery import timezone_recovery\n",
    "from prefect_lib.scraper.article_scraper import scraper as artcle_scraper\n",
    "from prefect_lib.scraper.publish_date_scraper import scraper as publish_date_scraper\n",
    "from prefect_lib.scraper.title_scraper import scraper as title_scraper\n",
    "from prefect_lib.settings import DEBUG_FILE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger: Logger = logging.getLogger('prefect.run.scrapying_deco')\n",
    "\n",
    "start_time: datetime = datetime.now()\n",
    "mongo: MongoModel = MongoModel()\n",
    "crawler_response: CrawlerResponseModel = CrawlerResponseModel(mongo)\n",
    "scraped_from_response: ScrapedFromResponseModel = ScrapedFromResponseModel(mongo)\n",
    "scraper_by_domain: ScraperInfoByDomainModel = ScraperInfoByDomainModel(mongo)\n",
    "controller: ControllerModel = ControllerModel(mongo)\n",
    "\n",
    "conditions: list = []\n",
    "urls: list[str] = [\n",
    "    'https://mainichi.jp/articles/20220605/k00/00m/030/136000c',\n",
    "]\n",
    "scrape_parm = [{\n",
    "    \"pattern\": 1,\n",
    "    \"css_selecter\": \"head > meta[name=\\\"pubdate\\\"]\",\n",
    "}]\n",
    "\n",
    "conditions.append({'url': {'$in': urls}})\n",
    "if conditions:\n",
    "    filter: Any = {'$and': conditions}\n",
    "else:\n",
    "    filter = None\n",
    "logger.info(f'=== crawler_responseへのfilter: {str(filter)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピング対象件数を確認\n",
    "record_count = crawler_response.count(filter=filter)\n",
    "print('=== 件数 ',record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records: Cursor = crawler_response.find(\n",
    "    projection=None,\n",
    "    filter=filter,\n",
    "    sort=[('domain', ASCENDING), ('response_time', ASCENDING)],\n",
    ")\n",
    "for record in records:\n",
    "    #print('record: ',record.keys())\n",
    "    print('record: ',record['url'])\n",
    "    # 各サイト共通の項目を設定\n",
    "    # response_bodyをbs4で解析\n",
    "    response_body: str = pickle.loads(record['response_body'])\n",
    "    #print('\\n\\n\\n',response_body)\n",
    "    soup: bs4 = bs4(response_body, 'lxml')\n",
    "    #page_source = soup.select_one('html')\n",
    "    #print('\\n\\n\\n',soup.select_one('html'))\n",
    "\n",
    "    path: str = os.path.join(\n",
    "        DEBUG_FILE_DIR, f'response_data.html')\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(str(soup.select_one('html')))\n",
    "\n",
    "    #print('\\n\\n\\n',response_body)\n",
    "\n",
    "    scrape_parm = sorted(scrape_parm, key=lambda d: d['pattern'], reverse=True)\n",
    "    print('\\n\\n=== scrape_parm ===', scrape_parm)\n",
    "\n",
    "    result = artcle_scraper(\n",
    "        soup=soup,\n",
    "        scraper='artcle_scraper',\n",
    "        scrape_parm=scrape_parm,\n",
    "    )\n",
    "    # result = publish_date_scraper(\n",
    "    #     soup=soup,\n",
    "    #     scraper='publish_scraper',\n",
    "    #     scrape_parm=scrape_parm,\n",
    "    # )\n",
    "    print('\\n\\n=== result ===', result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00b0d91d220cd2884303810c80f143c1222c3c3704eaa0756460e122a00ee18a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
